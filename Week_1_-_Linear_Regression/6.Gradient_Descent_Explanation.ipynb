{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for Linear Regression Explanation\n",
    "\n",
    "Automating the process of optimizing $w$ and $b$ using gradient descent.\n",
    "\n",
    "$$ \\min_{w, b} J(w, b) $$\n",
    "\n",
    "| Symbol | |\n",
    "| --- | --- |\n",
    "| w | The weight (or slope) of the linear regression model. |\n",
    "| b | The y intercept or bias of the linear regression model. |\n",
    "| J(w, b) | The cost function, typically the Mean Squared Error (MSE) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from utils_common import compute_model_output, plt_gradients\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simple data set\n",
    "x_train = np.array([2, 4, 6, 8, 10])   #features\n",
    "y_train = np.array([1, 3, 5, 7, 9])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec29d8d",
   "metadata": {},
   "source": [
    "#### Cost Function\n",
    "The cost function is a measure of how well the model fits the data. For linear regression, we typically use the Mean Squared Error (MSE) as the cost function:\n",
    "$$ J(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (wx_i + b))^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b): \n",
    "    # number of training examples\n",
    "    n = x.shape[0] \n",
    "    cost_sum = 0 \n",
    "    for i in range(n): \n",
    "        f_mb = w * x[i] + b   \n",
    "        cost = (f_mb - y[i]) ** 2  \n",
    "        cost_sum = cost_sum + cost  \n",
    "    total_cost = (1 / (2 * n)) * cost_sum  \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e0f5ac",
   "metadata": {},
   "source": [
    "#### Compute Gradient Function\n",
    "The gradient is the vector of partial derivatives of the cost function with respect to each parameter. For linear regression, we have:\n",
    "$$ \\nabla J(w, b) = \\begin{bmatrix} \\frac{\\partial J}{\\partial w} \\\\ \\frac{\\partial J}{\\partial b} \\end{bmatrix} $$\n",
    "\n",
    "A very simple explanation of how the gradient uses derivatives is to compute the gradient, we calculate the cost function with the current parameters and compare them with the cost function with the previous parameters. This gives us a derivative;\n",
    "\n",
    "1. If the derivative is positive, it means increasing the parameters will increase the cost, so we should decrease the parameters.\n",
    "2. If the derivative is negative, it means increasing parameters will decrease the cost, so we should increase the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost m.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost m.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    # Number of training examples\n",
    "    n = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(n):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / n \n",
    "    dj_db = dj_db / n \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b9f52",
   "metadata": {},
   "source": [
    "####  Gradient Descent Function\n",
    "The gradient descent algorithm updates the parameters in the direction of the negative gradient:\n",
    "$$ \\begin{bmatrix} w \\\\ b \\end{bmatrix} = \\begin{bmatrix} w \\\\ b \\end{bmatrix} - \\alpha \\nabla J(w, b) $$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n",
    "\n",
    "##### Learning Rate\n",
    "The learning rate is a hyperparameter that controls how much to change the model parameters in response to the estimated gradient. A small learning rate means the model will learn slowly, while a large learning rate can cause the model to converge too quickly to a suboptimal solution or even diverge.\n",
    "\n",
    "##### Stopping Criteria\n",
    "The stopping criteria for gradient descent can be based on:\n",
    "- A maximum number of iterations\n",
    "- A threshold for the change in the cost function\n",
    "- A threshold for the change in the parameters\n",
    "- A threshold for the gradient\n",
    "\n",
    "##### Goal\n",
    "The optimal parameters $w$ and $b$ that minimize the cost function $J(w, b)$ are found when convergence is reached because the partial derivative for both $w$ and $b$ are 0 before the 'stopping criteria'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1795966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    # An array to store cost J and m's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    data = pd.DataFrame(\n",
    "      {\n",
    "        \"Iteration\": [\"Initial\"],\n",
    "        \"Cost\": [f\"{cost_function(x, y, w , b):.8f}\"],\n",
    "        \"w part deriv\": [\"\"],\n",
    "        \"b part deriv\": [\"\"],\n",
    "        \"w\": [w_in],\n",
    "        \"b\": [b_in],\n",
    "      },\n",
    "      )\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:       \n",
    "            new_row = pd.DataFrame(\n",
    "            {\n",
    "              \"Iteration\": [i],\n",
    "              \"Cost\": [f\"{J_history[-1]:.8f}\"],\n",
    "              \"w part deriv\": [f\"{dj_dw:.8f}\"],\n",
    "              \"b part deriv\": [f\"{dj_db:.8f}\"],\n",
    "              \"w\": [w],\n",
    "              \"b\": [b],\n",
    "            },\n",
    "            )\n",
    "            data = pd.concat([data, new_row])\n",
    "    new_row = pd.DataFrame(\n",
    "      {\n",
    "        \"Iteration\": [\"Final\"],\n",
    "        \"Cost\": [f\"{J_history[-1]:.8f}\"],\n",
    "        \"w part deriv\": [f\"{dj_dw:.8f}\"],\n",
    "        \"b part deriv\": [f\"{dj_db:.8f}\"],\n",
    "        \"w\": [w],\n",
    "        \"b\": [b],\n",
    "      },\n",
    "      )\n",
    "    data = pd.concat([data, new_row])\n",
    "    return w, b, J_history, p_history, data #return wb and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8be4c",
   "metadata": {},
   "source": [
    "#### Set Gradient Descent Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = 0  #Starting w parameter\n",
    "b_init = 0  #Starting b parameter\n",
    "iterations = 10000  #Stopping criteria\n",
    "tmp_alpha = 0.010   #Learn rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35875e4",
   "metadata": {},
   "source": [
    "#### Run Gradient Descent And Analyze The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef15d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise Gradient Descent\n",
    "#Quiver plot, indicate magnitude and direction of Gradient\n",
    "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse gradient descent at various stages\n",
    "w_final, b_final, J_hist, p_hist, data = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, iterations, compute_cost, compute_gradient) \n",
    "print(data)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f794a",
   "metadata": {},
   "source": [
    "#### Plot The Optimal Parameters Discovered By Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_f_wb = compute_model_output(x_train, w_final, b_final)\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_train, tmp_f_wb, c='b',label='Our Prediction')\n",
    "plt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')\n",
    "plt.title('Apply the w,b found by gradient descent')\n",
    "plt.ylabel('Target')\n",
    "plt.xlabel('Feature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
