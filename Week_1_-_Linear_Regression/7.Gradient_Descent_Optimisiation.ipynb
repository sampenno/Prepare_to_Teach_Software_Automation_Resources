{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising Gradient Descent for Linear Regression\n",
    "\n",
    "Automating the process of optimizing $w$ and $b$ using gradient descent.\n",
    "\n",
    "$$ \\min_{w, b} J(w, b) $$\n",
    "\n",
    "| Symbol | |\n",
    "| --- | --- |\n",
    "| w | The weight (or slope) of the linear regression model. |\n",
    "| b | The y intercept (or bias) of the linear regression model. |\n",
    "| J(w, b) | The cost function, typically the Mean Squared Error (MSE) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.style.use('ggplot')\n",
    "from utils_grad_dec import plt_contour_mgrad, plt_divergence, plt_gradients, compute_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3cc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a simple data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec29d8d",
   "metadata": {},
   "source": [
    "#### Implement Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a915d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b): \n",
    "    # number of training examples\n",
    "    n = x.shape[0] \n",
    "    cost_sum = 0 \n",
    "    for i in range(n): \n",
    "        f_wb = w * x[i] + b   \n",
    "        cost = (f_wb - y[i]) ** 2  \n",
    "        cost_sum = cost_sum + cost  \n",
    "    total_cost = (1 / (2 * n)) * cost_sum  \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost m.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost m.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    # Number of training examples\n",
    "    n = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(n):  \n",
    "        f_wb = w * x[i] + b \n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / n \n",
    "    dj_db = dj_db / n \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    # An array to store cost J and m's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    data = pd.DataFrame(\n",
    "      {\n",
    "        \"Iteration\": [\"Initial\"],\n",
    "        \"Cost\": [f\"{cost_function(x, y, w , b):.6f}\"],\n",
    "        \"w part deriv\": [\"\"],\n",
    "        \"b part deriv\": [\"\"],\n",
    "        \"w\": [w_in],\n",
    "        \"b\": [b_in],\n",
    "      },\n",
    "      )\n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:       \n",
    "            new_row = pd.DataFrame(\n",
    "            {\n",
    "              \"Iteration\": [i],\n",
    "              \"Cost\": [f\"{J_history[-1]:.6f}\"],\n",
    "              \"w part deriv\": [f\"{dj_dw:.6f}\"],\n",
    "              \"b part deriv\": [f\"{dj_db:.6f}\"],\n",
    "              \"w\": [w],\n",
    "              \"b\": [b],\n",
    "            },\n",
    "            )\n",
    "            data = pd.concat([data, new_row])\n",
    "    new_row = pd.DataFrame(\n",
    "      {\n",
    "        \"Iteration\": [\"Final\"],\n",
    "        \"Cost\": [f\"{J_history[-1]:.6f}\"],\n",
    "        \"w part deriv\": [f\"{dj_dw:.6f}\"],\n",
    "        \"b part deriv\": [f\"{dj_db:.6f}\"],\n",
    "        \"w\": [w],\n",
    "        \"b\": [b],\n",
    "      },\n",
    "      )\n",
    "    data = pd.concat([data, new_row])\n",
    "    return w, b, J_history, p_history, data #return wb and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780d885",
   "metadata": {},
   "source": [
    "#### Visualise Gradient Decent\n",
    "\n",
    "Gradient descent optimisation reduces computational demand and increases intuition. Optimisation aims to achieve convergence more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe9123",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d512b9",
   "metadata": {},
   "source": [
    "#### Set Gradient Descent Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = 0  #Starting m parameter\n",
    "b_init = 0  #Starting b parameter\n",
    "iterations = 20000  #Stopping criteria\n",
    "tmp_alpha = 0.010   #Learn rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist, data = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, iterations, compute_cost, compute_gradient) \n",
    "print(data)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 200\n",
    "b = 100\n",
    "\n",
    "tmp_f_wb = compute_model_output(x_train, w, b,)\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_train, tmp_f_wb, c='b',label='Our Prediction')\n",
    "plt.scatter(x_train, y_train, marker='x', c='r',label='Actual Values')\n",
    "plt.title('Apply the w,b found by gradient descent')\n",
    "plt.ylabel('Target')\n",
    "plt.xlabel('Feature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e364d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with w,b found by gradient descent\n",
    "print(f\"Feature = 1 prediction {w_final*1.0 + b_final:0.1f}\")\n",
    "print(f\"Feature = 1.2 prediction {w_final*1.2 + b_final:0.1f}\")\n",
    "print(f\"Feature = 2 prediction {w_final*2.0 + b_final:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92db7a4c",
   "metadata": {},
   "source": [
    "#### Optimal Progress of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b01cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost versus iteration  \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12,4))\n",
    "ax1.plot(J_hist[:100])\n",
    "ax2.plot(1000 + np.arange(len(J_hist[1000:])), J_hist[1000:])\n",
    "ax1.set_title(\"Cost vs. iteration(start)\");  ax2.set_title(\"Cost vs. iteration (end)\")\n",
    "ax1.set_ylabel('Cost')            ;  ax2.set_ylabel('Cost') \n",
    "ax1.set_xlabel('iteration step')  ;  ax2.set_xlabel('iteration step') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12, 4))\n",
    "plt_contour_mgrad(x_train, y_train, p_hist, ax, m_range=[180, 220, 0.5], b_range=[80, 120, 0.5],\n",
    "            contours=[1,5,10,20],resolution=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c830a52",
   "metadata": {},
   "source": [
    "#### Learning Rate & Stopping Criteria\n",
    "\n",
    "Gradient Descent uses the learning rate as a value to increment $w$ and $b$ with each iteration. Whilst a larger learning rate can optimise Gradient Descent, it can result in an overshot. Also, a too-small learning rate can cause Gradient Descent to take too long to find convergence.\n",
    "\n",
    "Stopping criteria are rules or conditions used to decide when to stop the iterative process of gradient descent. Since gradient descent is an iterative algorithm that updates parameters to minimize a loss function, it needs a clear rule to know when to stop. Stopping criteria can also stop over fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61355603",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = 0 #Starting m parameter\n",
    "b_init = 0 #Starting b parameter\n",
    "iterations = 10 #Stopping criteria\n",
    "tmp_alpha = 0.8 #Learn rate\n",
    "w_final, b_final, J_hist, p_hist, data = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, iterations, compute_cost, compute_gradient) \n",
    "print(data)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_divergence(p_hist, J_hist,x_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464fa148",
   "metadata": {},
   "source": [
    "#### Data normalisation\n",
    "\n",
    "Normalising data ensures that all features contribute equally and gradient descent works efficiently and stably, leading to faster and more reliable training of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize x_train and y_train to optimise gradient descent\n",
    "x_min, x_max = x_train.min(), x_train.max()\n",
    "y_min, y_max = y_train.min(), y_train.max()\n",
    "\n",
    "x_train_norm = (x_train - x_min) / (x_max - x_min)\n",
    "y_train_norm = (y_train - y_min) / (y_max - y_min)\n",
    "\n",
    "print(\"Normalized x_train:\", x_train_norm)\n",
    "print(\"Normalized y_train:\", y_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "w_init_norm = 0\n",
    "b_init_norm = 0\n",
    "# some gradient descent settings\n",
    "iterations_norm = 3000\n",
    "tmp_alpha_norm = 0.1\n",
    "\n",
    "w_final, b_final, J_hist, p_hist, data = gradient_descent(x_train_norm ,y_train_norm, w_init_norm, b_init_norm, tmp_alpha_norm, iterations_norm, compute_cost, compute_gradient) \n",
    "print(data)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 4))\n",
    "plt_contour_mgrad(x_train_norm, y_train_norm, p_hist, ax, m_range=[-2, 2, 0.1], b_range=[-2, 2, 0.1],\n",
    "            contours=[0,0.2,0.4,0.6,0.8],resolution=0.05, w_final=w_final, b_final=b_final)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5636bd",
   "metadata": {},
   "source": [
    "#### Multiple minima also known as local cost versus global cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent with Non MSE Cost Function with multiple minima\n",
    "x = np.linspace(-10, 10, 500)\n",
    "y = np.linspace(-10, 10, 500)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = (X**2 + Y**2) * np.sin(X / 2) * np.sin(Y / 2) + 10 * np.exp(-((X - 5)**2 + (Y - 5)**2)) \\\n",
    "    + 8 * np.exp(-((X + 5)**2 + (Y + 5)**2))\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
    "ax.set_title(\"Surface Plot with Muktiple Lowest Cost Points\")\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "ax.set_zlabel(\"Target\")\n",
    "fig.colorbar(surf, shrink=0.5, aspect=10)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "dl_toc_settings": {
   "rndtag": "40291"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
