{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902f62a8",
   "metadata": {},
   "source": [
    "# Advanced Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ec67a",
   "metadata": {},
   "source": [
    "### K-nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b676a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from utils_common import generate_data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c0cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random data set\n",
    "m = generate_data(0, 50, 0, 50, 150, 0.4)\n",
    "n = generate_data(0, 50, 0, 50, 150, 0.45)\n",
    "o = generate_data(30, 50, 30, 50, 10, 0.1)\n",
    "\n",
    "cols = [random.randint(0, 1) for _ in range(10)]\n",
    "radii = [1, 2, 3, 4, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea05144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour Regression\n",
    "# Mean of the K-Nearest Neighbours is used as the prediction\n",
    "zoom = True\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.scatter(40, 40, color='red', label='Target Point')\n",
    "if not zoom:\n",
    "    plt.scatter(m[0], m[1], color='black')\n",
    "    plt.scatter(n[0], n[1], color='black')\n",
    "else:\n",
    "    plt.scatter(o[0], o[1], color='black')\n",
    "    plt.plot([],[],color='green', linestyle='--', linewidth=1, label='Euclidean Distance')\n",
    "    for xi, yi in zip(o[0], o[1]):\n",
    "        plt.plot([40, xi], [40, yi], color='green', linestyle='--', linewidth=1)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"K-Nearest Neighbour\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565cbde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbour Classification\\\n",
    "# Mode Class of the K-Nearest Neighbours is used as the prediction\n",
    "zoom = True\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(40, 40, color='red', label='Target Point')\n",
    "if not zoom:\n",
    "    plt.scatter(m[0], m[1], color='gold', label='Class 0')\n",
    "    plt.scatter(n[0], n[1], color='indigo', label='Class 1')\n",
    "else:\n",
    "    plt.scatter([], [], c='gold', label='Class 0')\n",
    "    plt.scatter([], [], c='indigo', label='Class 1')\n",
    "    plt.scatter(o[0], o[1], c=cols)\n",
    "    plt.plot([],[],color='green', linestyle='--', linewidth=1, label='Euclidean Distance')\n",
    "    for radius in radii:\n",
    "        circle = Circle((40, 40), radius, color='blue', fill=False, linestyle='--')\n",
    "        ax.add_patch(circle)\n",
    "    for xi, yi in zip(o[0], o[1]):\n",
    "        plt.plot([40, xi], [40, yi], color='green', linestyle='--', linewidth=1)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "plt.legend()\n",
    "plt.title(\"K-Nearest Neighbour Classification\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bafd0d",
   "metadata": {},
   "source": [
    "#### Measuring Distance\n",
    "\n",
    "KNN uses either Euclidean distance (straight-line) and Manhattan distance (grid-like, right-angle path) between two points. Although Euclidean distance is the most commenly used measure in KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Euclidean & Manhattan Distance\n",
    "A, B = np.array([2, 3]), np.array([8, 7])\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(*A, color='blue', s=100, label='A')\n",
    "ax.scatter(*B, color='red', s=100, label='B')\n",
    "# Euclidean (straight line)\n",
    "ax.plot([A[0], B[0]], [A[1], B[1]], color='green', lw=2, label='Euclidean')\n",
    "# Manhattan (right-angle path)\n",
    "ax.plot([A[0], B[0]], [A[1], A[1]], color='orange', ls='--', lw=2)\n",
    "ax.plot([B[0], B[0]], [A[1], B[1]], color='orange', ls='--', lw=2, label='Manhattan')\n",
    "ax.annotate('A', A + [0.2, -0.2], fontsize=12, color='blue')\n",
    "ax.annotate('B', B + [0.2, 0.2], fontsize=12, color='red')\n",
    "ax.grid(True, linestyle=':')\n",
    "ax.set(xlim=(0, 10), ylim=(0, 10), aspect='equal', xlabel='X', ylabel='Y', title='Euclidean vs Manhattan Distance')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b81c1",
   "metadata": {},
   "source": [
    "### Neural Network Course Specifications\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"images\\NN_Course-Specs.png\" alt=\"Course Specs Neural Network image\" width=\"500\" />\n",
    "    <figcaption><p><em>Source: Page 29 of the Software Engineering Course Specifications</em></p>\n",
    "    </figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Neural networks were designed to mimic the processing inside the human brain. They consist of a series of interconnected nodes (artificial neurones). Each neurone can accept a binary input signal and potentially output another signal to connected nodes.\n",
    "\n",
    "#### Training cycle\n",
    "\n",
    "Internal weightings and threshold values for each node are determined in the initial training cycle for each neural network. The system is exposed to a series of inputs with known responses. Linear regression with backward chaining is used to iteratively determine the set of unique values required for output. Regular exposure to the training cycle results in improved accuracy and pattern matching.\n",
    "\n",
    "#### Execution cycle\n",
    "\n",
    "In the diagram, signal strength between nodes with the strongest weightings are thicker representing a higher priority in determining the final output. The execution cycle follows the training cycle and utilises the internal values developed during the training cycle to determine the output.\n",
    "\n",
    "Page 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776f489",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"images\\decision_tree.png\" alt=\"Decision Tree\" width=\"500\" />\n",
    "</figure>\n",
    "\n",
    "1. Root Node:\n",
    "The algorithm starts with the entire dataset as the root node. \n",
    "2. Splitting:\n",
    "At each node, the algorithm selects the feature and split value that best separates the data into subsets, minimizing the variance or impurity of the target variable in the subsets. \n",
    "3. Recursion:\n",
    "This process is repeated for each subset, creating new nodes and branches until a stopping criterion is met (e.g., maximum tree depth, minimum number of samples in a leaf). \n",
    "4. Leaf Nodes:\n",
    "The leaf nodes contain the predicted values, which are often the average or mean of the target variable values in the corresponding subset. \n",
    "5. Prediction:\n",
    "To predict the value for a new data point, you follow the path from the root node to a leaf node based on the data point's features, and the prediction is the value stored in that leaf. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
