{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils_common import dlc, plot_data, draw_vthresh, compute_model_output, generate_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we use Linear Regression for classification?\n",
    "o = generate_data(-10, 2.5, 0, 0, 300, 0.8)\n",
    "n = generate_data(-2.5, 10, 1, 1, 300, 0.8)\n",
    "\n",
    "w = 0.05\n",
    "b = 0.5\n",
    "tmp_f_wb = compute_model_output(np.array([-10, 10]), w, b,)\n",
    "\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.scatter(o[0], o[1], color='red')\n",
    "plt.scatter(n[0], n[1], color='blue')\n",
    "plt.plot(np.array([-10, 10]), tmp_f_wb, c='g',label='Our Prediction')\n",
    "plt.grid(True)\n",
    "plt.title(\"Linear Regression for Classification?\")\n",
    "# Add decision boundary at x=0\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=2, label='Decision Boundary (x=0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise classification\n",
    "x_train = np.array([-5, -4, -1, 0, 3, 4, 7, 8])\n",
    "y_train = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "X_train2 = np.array([[0.5, 1.5], [1, 1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train2 = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n",
    "pos = y_train == 1\n",
    "neg = y_train == 0\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,3))\n",
    "#plot 1, single variable\n",
    "ax[0].scatter(x_train[pos], y_train[pos], marker='x', s=80, c = 'red', label=\"y=1\")\n",
    "ax[0].scatter(x_train[neg], y_train[neg], marker='o', s=100, label=\"y=0\", facecolors='none', edgecolors=dlc[\"dlblue\"],lw=3)\n",
    "\n",
    "ax[0].set_ylim(-0.08,1.1)\n",
    "ax[0].set_ylabel('y', fontsize=12)\n",
    "ax[0].set_xlabel('x', fontsize=12)\n",
    "ax[0].set_title('Single feature plot')\n",
    "ax[0].legend()\n",
    "\n",
    "#plot 2, two variables\n",
    "plot_data(X_train2, y_train2, ax[1] )\n",
    "ax[1].axis([0, 4, 0, 4])\n",
    "ax[1].set_ylabel('$x_1$', fontsize=12)\n",
    "ax[1].set_xlabel('$x_0$', fontsize=12)\n",
    "ax[1].set_title('Two feature plot (clustering)')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple sigmoid function\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "| Symbol | |\n",
    "| --- | --- |\n",
    "| $z$ | is the input to the function. |\n",
    "| $g(z)$ | maps any real-val |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!Note]\n",
    "> The Sigmoid function is the most used function in Logistic Regression as it has an output between 0 and 1. The Tahn function is also use when an output range between -1 and 1 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function & Decision Boundary\n",
    "def sigmoid(z):\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g\n",
    "z_tmp = np.arange(-10,10)\n",
    "y = sigmoid(z_tmp)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,3))\n",
    "ax.plot(z_tmp, y, c=\"b\")\n",
    "\n",
    "ax.set_title(\"Sigmoid Function and Decision Boundary for one feature\")\n",
    "ax.set_ylabel('Target')\n",
    "ax.set_xlabel('Feature')\n",
    "ax.grid(True)\n",
    "draw_vthresh(ax,0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple logistic function\n",
    "\n",
    "$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot  \\mathbf{x}^{(i)} + b)$, to predict $y$ given $x$\n",
    "\n",
    "<span style=\"color : red\">Students are not expected to recall this function, rather they should focus on the underlying concepts of the Logistic Regression algorythm.</span>\n",
    "\n",
    "| Symbol | |\n",
    "| --- | --- |\n",
    "| $f$ | A function indicating a rule that assigns a unique output value (y) for each input value (x) |\n",
    "| $w$ | Weight is the importance or influence of each feature on the prediction |\n",
    "| $b$ | Bias is a constant that shifts the decision boundary |\n",
    "| $x$ | Feature(s) |\n",
    "| $g$ | The sigmoid function and it maps all input values to values between 0 and 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function & Decision Boundary\n",
    "# Visualise Loss Function\n",
    "# Manual fitting of the Sigmoid Function\n",
    "\n",
    "decsion_boundary = 1.5\n",
    "def sigmoid(z):\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g\n",
    "z_tmp = np.arange(-10,10)\n",
    "y = sigmoid(z_tmp)\n",
    "fig,ax = plt.subplots(1,1)\n",
    "math = r'$ g(z) = \\frac{1}{1 + e^{-z}} $'\n",
    "ax.plot(z_tmp + decsion_boundary, y, c=\"b\", label=f\"Sigmoid Function \\n{math}\")\n",
    "ax.set_title(\"Logistic Regression for Binary Classification\")\n",
    "ax.set_ylabel('Target')\n",
    "ax.set_xlabel('Feature')\n",
    "ax.grid(True)\n",
    "draw_vthresh(ax, decsion_boundary)\n",
    "ax.scatter(x_train, y_train, s=100, c=y_train, zorder=10)\n",
    "plt.axvline(x=decsion_boundary, color='black', linestyle='--', linewidth=1, label=f'Decision Boundary\\n (z={decsion_boundary} & y=0.5)')\n",
    "# Draw vertical lines to show the loss for each data point\n",
    "for xi, yi in zip(x_train, y_train):\n",
    "    y_pred = sigmoid(xi - decsion_boundary)\n",
    "    ax.plot([xi, xi], [yi, y_pred], color='green', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.plot([],[],color='green', linestyle='--', linewidth=1, label='Logistic Loss')\n",
    "target_y = 0.818\n",
    "z_target = -np.log((1 / target_y) - 1)  # Inverse of sigmoid\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Decision Boundary for two features\n",
    "\n",
    "x0 = np.arange(0,6)\n",
    "x1 = 3 - x0\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
    "ax.set_title(\"Decision Boundary for two features\")\n",
    "ax.plot(x0,x1, c=\"b\")\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.fill_between(x0,x1, alpha=0.2)\n",
    "plot_data(X_train2, y_train2,ax)\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you have a sigmoid curve that defines a decision boundary, what do you think are the next steps in the Logistic Regression algorythm?\n",
    "\n",
    "<details>\n",
    "    <summary><h3 style=\"display:inline\">Optional mathematical functions</h3></summary>\n",
    "\n",
    "<span style=\"color : red\">Students are not expected to recall or be able to \n",
    "calculate these functions, they are shown for those interested in the mathematics of the algorythms.</span>\n",
    "\n",
    "#### Logistic Lost\n",
    "\n",
    "$$ \\sum_{i=0}^{m-1} - (y _i∗log(p _i)+(1−y _i  )∗log(1−p _i  )) $$\n",
    "\n",
    "#### Logistic Cost Function\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] $$\n",
    "\n",
    "#### Gradient Decent\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}   \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
