{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils_common import compute_model_output, generate_data\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random data set\n",
    "m = generate_data(25, 50, 25, 50, 100, 0.9)\n",
    "n = generate_data(0, 50, 0, 50, 10, 0.01)\n",
    "w = 1\n",
    "b = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biased data?\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "tmp_f_wb = compute_model_output(np.concatenate([m[0], n[0]]), w, b,)\n",
    "plt.plot(np.concatenate([m[0], n[0]]), tmp_f_wb, c='r', label=f'Prediction: y = {w}x + {b}' )\n",
    "plt.scatter(m[0], m[1], color='pink', label='Women')\n",
    "plt.scatter(n[0], n[1], color='blue', label='Men')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in Machine Learning\n",
    "\n",
    "Machine learning bias, also called algorithmic or AI bias, is the tendency of a model to exhibit a preference or prejudice for certain outcomes or groups over others. Do not confuse it with model bias, I know right!\n",
    "\n",
    "Bias is extremely complex and can occur at many points in Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "## How bias occurs in each stage of MLOPS\n",
    "\n",
    "```mermaid\n",
    "    stateDiagram-v2\n",
    "        direction LR\n",
    "        state Design {\n",
    "            Motivation --> Data: bias\n",
    "            Data --> Feature#32;Engineering: bias\n",
    "        }\n",
    "        state Development {\n",
    "            Feature#32;Engineering --> Model: bias\n",
    "            Model --> Interpretation: bias\n",
    "        }\n",
    "        state Operations {\n",
    "            Interpretation --> Application: bias\n",
    "        }\n",
    "        Application  --> [*]: Biased ML/AI\n",
    "```\n",
    "\n",
    "### Design\n",
    "\n",
    "**Business Problem**: When the business problem is inherantly bias.\n",
    "\n",
    "**Refactoring to ML Problem**: When ML problem is refactored and bias is introduced, reinforced or amplified.\n",
    "\n",
    "**Data**: When raw data is improperly selected, incomplete or untrue in any form.\n",
    "\n",
    "### Development\n",
    "\n",
    "**Data Wrangling**: When data is cleaned and transformed prior to being ingested into a model for training, such as missing values or rounding errors and bias is introduced, reinforced or amplified.\n",
    "\n",
    "**Feature Engineering**: When features are selected or engineered with bias or that amplify inherant bias.\n",
    "\n",
    "**Model Selection**: Bias can be amplified in the selection of the actual models or algorithms such as classification versus regression; some algorithms are more sensitive to bias than others.\n",
    "\n",
    "**Model Fit and Refinement**: When models are coded, if needed, and undergo training and testing, errors in algorithms can impact bias.\n",
    "\n",
    "**Model Evaluation** When there is insufficient or biased validation. Such as the when the training data is insufficient, absent or the training testing split aplifies or creates bias.\n",
    "\n",
    "### Operations\n",
    "\n",
    "**Operations**: Improper human feedback or monitoring can drive errors in the model, reducing its effectiveness or allowing the model to degrade overtime.\n",
    "\n",
    "**Interpretation**: User will apply their own interpretations and opinions about the predicted output. Users who don't trust or \"like\" the predicted output will insert their own errors and biases, reducing the model's value.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of Bias\n",
    "\n",
    "**Algorithm bias**: This occurs when there's a problem within the algorithm that performs the calculations or other processing that powers the ML computations.\n",
    "\n",
    "**Automation bias**: This occurs when the results of automated systems are preferred over the results of human or other non-automated systems, even though the automated system might not provide better accuracy. In other words, users trust AI more.\n",
    "\n",
    "**Sample bias**: This happens when there's a problem with the data used to train the ML model. In this type of bias, the data used either isn't large enough or representative enough to teach the system. For example, using training data that features only female teachers trains the system to conclude that all teachers are female.\n",
    "\n",
    "**Prejudice bias**: In this case, the data used to train the system reflects existing prejudices, stereotypes and faulty societal assumptions, thereby introducing those same real-world biases into the machine learning itself. For example, using data about medical professionals that includes only female nurses and male doctors could perpetuate a real-world gender stereotype about healthcare workers in the computer system.\n",
    "\n",
    "**Implicit bias**: Similar to prejudice bias, implicit bias occurs when models are designed or data is curated using the human designer's own ways of thinking or personal experiences, which might not fully or accurately map to the task at hand.\n",
    "Group attribution bias. This occurs when the characteristics of an individual or single sample are improperly applied to a broader set of individuals or a group of data points. Such generalizations made about an entire group can overlook the nuances of individual samples.\n",
    "\n",
    "**Measurement bias**: As the name suggests, this bias arises due to underlying problems with the accuracy of the data and how it was measured or assessed. Using pictures of happy workers to train a system meant to assess a workplace environment could be biased if the workers in the pictures knew they were being measured for happiness; a system being trained to precisely assess weight is biased if the weights contained in the training data were consistently rounded up or down.\n",
    "\n",
    "**Exclusion or reporting bias**: This happens when an important data point is left out of the data being used. This can happen if the modelers don't recognize the data point as consequential. For example, incidents reported in police crime analytics might be skewed when incidents go unreported or under-reported because victims fail to report the incidents.\n",
    "\n",
    "**Selection bias**: This occurs when the data used in training either isn't large enough or representative enough, thereby misrepresenting and lowering accuracy results and performance. There are several variations of selection bias, including coverage bias where the data isn't representative, participation bias where non-responses leave gaps in data, and sampling bias where statistical randomization isn't used.\n",
    "\n",
    "**Recall bias**: This data quality bias develops in the data labeling stage, where labels are inconsistently given through subjective observations. Recall is measured as how many points are labeled accurately over the total number of observations in a model.\n",
    "\n",
    "**Gender Bias**: LLMs can give bias in the output when the model associates specific traits, roles, or behaviors predominantly with a particular gender. For example, associating roles like “nurse” with women or providing gender stereotypical sentences such as “she is a homemaker” in response to ambiguous prompts.\n",
    "\n",
    "**Socioeconomic Bias**: Socioeconomic bias happens when the model associates certain behaviors or values with a specific economic class or profession. For example, the model output provides that “successful” is primarily only about white-collar occupations.\n",
    "\n",
    "**Ability Bias**: Bias occurs when the model outputs stereotypes or negative associations regarding individuals with disabilities. If the model produces this result, offensive language shows bias.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
